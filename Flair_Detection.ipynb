{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining The Flair Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"[R]eddiquette\", \n",
    "          \"Scheduled\", \"Photography\", \"Science/Technology\",\n",
    "          \"Politics\", \"Business/Finance\", \"Policy/Economy\",\n",
    "          \"Sports\", \"Food\", \"AMA\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 author                                            authors  \\\n0         dhavalcoholic                                    ICICIPruLifeIns   \n1       amitkumarthakur   RAD-Business RAD-Business None barcam10 _snor...   \n2  FrustratedOCIHopeful   plshelpthedog ayyylmaaaoo Proper_Boysenberry ...   \n3        aloo_vs_bhaloo   vcdarklord tilismilis aloo_vs_bhaloo dogaa fo...   \n4             multubunu                                                NaN   \n\n                                                body  \\\n0  reposting lack activity r askindiahello last y...   \n1  24hrs local police station register case dont ...   \n2  hello askindia first time poster long time lur...   \n3                     r tooafraidtoask india edition   \n4  hello submitted r raskindia week ago got answe...   \n\n                                             comment  comms_num       created  \\\n0  dear policy holder dhavalcoholic request help ...          1  1.386254e+09   \n1  calm downgo sp office town file grievance imme...         24  1.554080e+09   \n2  honestly supervisor behaved exactly government...         27  1.555361e+09   \n3  modi control sex desires jerk someone else pro...         22  1.566529e+09   \n4                                                NaN          0  1.361085e+09   \n\n      flair      id  score                                              title  \\\n0  AskIndia  1s57oi      1  need feedback insurance policy took xpost aski...   \n1  AskIndia  b7pvwt     94                     somebody want kill full family   \n2  AskIndia  bdfid1     10  ambassador india takes back newly issued oci c...   \n3  AskIndia  cu1xn4     18                                randians afraid ask   \n4  AskIndia  18ntue      0                    askindia cingari cengar tzengar   \n\n                                                 url  \\\n0  https://www.reddit.com/r/india/comments/1s57oi...   \n1  https://www.reddit.com/r/india/comments/b7pvwt...   \n2  https://www.reddit.com/r/india/comments/bdfid1...   \n3  https://www.reddit.com/r/india/comments/cu1xn4...   \n4  https://www.reddit.com/r/india/comments/18ntue...   \n\n                                   combined_features  \n0  need feedback insurance policy took xpost aski...  \n1  somebody want kill full familycalm downgo sp o...  \n2  ambassador india takes back newly issued oci c...  \n3  randians afraid askmodi control sex desires je...  \n4  askindia cingari cengar tzengarhttps://www.red...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author</th>\n      <th>authors</th>\n      <th>body</th>\n      <th>comment</th>\n      <th>comms_num</th>\n      <th>created</th>\n      <th>flair</th>\n      <th>id</th>\n      <th>score</th>\n      <th>title</th>\n      <th>url</th>\n      <th>combined_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>dhavalcoholic</td>\n      <td>ICICIPruLifeIns</td>\n      <td>reposting lack activity r askindiahello last y...</td>\n      <td>dear policy holder dhavalcoholic request help ...</td>\n      <td>1</td>\n      <td>1.386254e+09</td>\n      <td>AskIndia</td>\n      <td>1s57oi</td>\n      <td>1</td>\n      <td>need feedback insurance policy took xpost aski...</td>\n      <td>https://www.reddit.com/r/india/comments/1s57oi...</td>\n      <td>need feedback insurance policy took xpost aski...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>amitkumarthakur</td>\n      <td>RAD-Business RAD-Business None barcam10 _snor...</td>\n      <td>24hrs local police station register case dont ...</td>\n      <td>calm downgo sp office town file grievance imme...</td>\n      <td>24</td>\n      <td>1.554080e+09</td>\n      <td>AskIndia</td>\n      <td>b7pvwt</td>\n      <td>94</td>\n      <td>somebody want kill full family</td>\n      <td>https://www.reddit.com/r/india/comments/b7pvwt...</td>\n      <td>somebody want kill full familycalm downgo sp o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FrustratedOCIHopeful</td>\n      <td>plshelpthedog ayyylmaaaoo Proper_Boysenberry ...</td>\n      <td>hello askindia first time poster long time lur...</td>\n      <td>honestly supervisor behaved exactly government...</td>\n      <td>27</td>\n      <td>1.555361e+09</td>\n      <td>AskIndia</td>\n      <td>bdfid1</td>\n      <td>10</td>\n      <td>ambassador india takes back newly issued oci c...</td>\n      <td>https://www.reddit.com/r/india/comments/bdfid1...</td>\n      <td>ambassador india takes back newly issued oci c...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>aloo_vs_bhaloo</td>\n      <td>vcdarklord tilismilis aloo_vs_bhaloo dogaa fo...</td>\n      <td>r tooafraidtoask india edition</td>\n      <td>modi control sex desires jerk someone else pro...</td>\n      <td>22</td>\n      <td>1.566529e+09</td>\n      <td>AskIndia</td>\n      <td>cu1xn4</td>\n      <td>18</td>\n      <td>randians afraid ask</td>\n      <td>https://www.reddit.com/r/india/comments/cu1xn4...</td>\n      <td>randians afraid askmodi control sex desires je...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>multubunu</td>\n      <td>NaN</td>\n      <td>hello submitted r raskindia week ago got answe...</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1.361085e+09</td>\n      <td>AskIndia</td>\n      <td>18ntue</td>\n      <td>0</td>\n      <td>askindia cingari cengar tzengar</td>\n      <td>https://www.reddit.com/r/india/comments/18ntue...</td>\n      <td>askindia cingari cengar tzengarhttps://www.red...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(\"\",inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "unable to import 'smart_open.gcs', disabling that module\n"
    }
   ],
   "source": [
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean_text(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "data.body = data.body.apply(lambda x:clean_text(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##logistic regression\n",
    "def logisticreg(X_train, X_test, y_train, y_test):\n",
    "\n",
    "  from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "  logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "                 ])\n",
    "  logreg.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = logreg.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## naive bayes\n",
    "def nb_classifier(X_train, X_test, y_train, y_test):\n",
    "  nb = Pipeline([('vect', CountVectorizer()),\n",
    "                 ('tfidf', TfidfTransformer()),\n",
    "                 ('clf', MultinomialNB()),\n",
    "                ])\n",
    "  nb.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = nb.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVM\n",
    "def linear_svm(X_train, X_test, y_train, y_test):\n",
    "  sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "                 ])\n",
    "  sgd.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = sgd.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## random forest\n",
    "def randomforest(X_train, X_test, y_train, y_test):\n",
    "  ranfor = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', RandomForestClassifier(n_estimators = 1000, random_state = 42)),\n",
    "                 ])\n",
    "  ranfor.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = ranfor.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mlp\n",
    "def mlpclassifier(X_train, X_test, y_train, y_test):  \n",
    "  mlp = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', MLPClassifier(hidden_layer_sizes=(30,30,30))),\n",
    "                 ])\n",
    "  mlp.fit(X_train, y_train)\n",
    "\n",
    "  y_pred = mlp.predict(X_test)\n",
    "\n",
    "  print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "  print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgboost\n",
    "def xgbclassifier(X_train, X_test, y_train, y_test):  \n",
    "    xgb_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                  ('tfidf', TfidfTransformer()),\n",
    "                  ('clf', XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7,objective='multi:softmax')),\n",
    "                 ])\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "    print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "    print(classification_report(y_test, y_pred,target_names=flairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(X,y):\n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "   \n",
    "    print(\"Results of Naive Bayes Classifier\")\n",
    "    nb_classifier(X_train, X_test, y_train, y_test)\n",
    "    print(\"Results of Linear Support Vector Machine\")\n",
    "    linear_svm(X_train, X_test, y_train, y_test)\n",
    "    print(\"Results of Logistic Regression\")\n",
    "    logisticreg(X_train, X_test, y_train, y_test)\n",
    "    print(\"Results of Random Forest\")\n",
    "    randomforest(X_train, X_test, y_train, y_test)\n",
    "    print(\"Results of MLP Classifier\")\n",
    "    mlpclassifier(X_train, X_test, y_train, y_test)\n",
    "    print(\"Results of XGB Classifier\")\n",
    "    xgbclassifier(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be checking the accuracy with title,URL,body,comments and combination of them as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Flair Detection using Title as Feature\nResults of Naive Bayes Classifier\naccuracy 0.8571428571428571\n                    precision    recall  f1-score   support\n\n          AskIndia       0.85      0.85      0.85        34\n     Non-Political       0.61      0.93      0.74        30\n     [R]eddiquette       0.82      0.75      0.79        44\n         Scheduled       0.97      0.87      0.92        38\n       Photography       0.88      0.93      0.90        45\nScience/Technology       0.94      0.96      0.95        47\n          Politics       0.82      0.94      0.88        34\n  Business/Finance       0.81      0.81      0.81        37\n    Policy/Economy       0.88      0.91      0.89        46\n            Sports       0.95      0.79      0.86        48\n              Food       0.94      0.86      0.90        37\n               AMA       0.00      0.00      0.00         8\n\n          accuracy                           0.86       448\n         macro avg       0.79      0.80      0.79       448\n      weighted avg       0.85      0.86      0.85       448\n\nResults of Linear Support Vector Machine\naccuracy 0.8995535714285714\n                    precision    recall  f1-score   support\n\n          AskIndia       0.91      0.88      0.90        34\n     Non-Political       0.85      0.93      0.89        30\n     [R]eddiquette       0.88      0.80      0.83        44\n         Scheduled       0.92      0.87      0.89        38\n       Photography       0.98      0.98      0.98        45\nScience/Technology       0.94      0.96      0.95        47\n          Politics       0.92      0.97      0.94        34\n  Business/Finance       0.79      0.84      0.82        37\n    Policy/Economy       0.84      0.91      0.87        46\n            Sports       0.96      0.90      0.92        48\n              Food       0.89      0.86      0.88        37\n               AMA       1.00      0.88      0.93         8\n\n          accuracy                           0.90       448\n         macro avg       0.91      0.90      0.90       448\n      weighted avg       0.90      0.90      0.90       448\n\nResults of Logistic Regression\naccuracy 0.9017857142857143\n                    precision    recall  f1-score   support\n\n          AskIndia       0.88      0.85      0.87        34\n     Non-Political       0.88      0.93      0.90        30\n     [R]eddiquette       0.81      0.80      0.80        44\n         Scheduled       0.92      0.92      0.92        38\n       Photography       1.00      0.98      0.99        45\nScience/Technology       0.96      1.00      0.98        47\n          Politics       0.94      0.97      0.96        34\n  Business/Finance       0.84      0.84      0.84        37\n    Policy/Economy       0.86      0.91      0.88        46\n            Sports       0.91      0.88      0.89        48\n              Food       0.91      0.86      0.89        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.90       448\n         macro avg       0.90      0.89      0.89       448\n      weighted avg       0.90      0.90      0.90       448\n\nResults of Random Forest\naccuracy 0.9040178571428571\n                    precision    recall  f1-score   support\n\n          AskIndia       0.94      0.91      0.93        34\n     Non-Political       0.78      0.93      0.85        30\n     [R]eddiquette       0.95      0.80      0.86        44\n         Scheduled       0.87      0.89      0.88        38\n       Photography       0.96      0.98      0.97        45\nScience/Technology       0.94      0.96      0.95        47\n          Politics       0.85      0.97      0.90        34\n  Business/Finance       0.85      0.95      0.90        37\n    Policy/Economy       0.95      0.87      0.91        46\n            Sports       1.00      0.83      0.91        48\n              Food       0.84      0.86      0.85        37\n               AMA       0.89      1.00      0.94         8\n\n          accuracy                           0.90       448\n         macro avg       0.90      0.91      0.90       448\n      weighted avg       0.91      0.90      0.90       448\n\nResults of MLP Classifier\naccuracy 0.8816964285714286\n                    precision    recall  f1-score   support\n\n          AskIndia       0.91      0.85      0.88        34\n     Non-Political       0.82      0.93      0.87        30\n     [R]eddiquette       0.78      0.80      0.79        44\n         Scheduled       0.91      0.82      0.86        38\n       Photography       0.86      0.96      0.91        45\nScience/Technology       0.98      0.96      0.97        47\n          Politics       0.86      0.91      0.89        34\n  Business/Finance       0.88      0.78      0.83        37\n    Policy/Economy       0.88      0.91      0.89        46\n            Sports       0.96      0.92      0.94        48\n              Food       0.86      0.86      0.86        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.88       448\n         macro avg       0.88      0.87      0.87       448\n      weighted avg       0.88      0.88      0.88       448\n\nResults of XGB Classifier\naccuracy 0.8080357142857143\n                    precision    recall  f1-score   support\n\n          AskIndia       0.84      0.79      0.82        34\n     Non-Political       0.82      0.90      0.86        30\n     [R]eddiquette       0.88      0.68      0.77        44\n         Scheduled       0.86      0.82      0.84        38\n       Photography       1.00      0.98      0.99        45\nScience/Technology       0.88      0.89      0.88        47\n          Politics       0.78      0.85      0.82        34\n  Business/Finance       0.52      0.86      0.65        37\n    Policy/Economy       0.92      0.76      0.83        46\n            Sports       0.82      0.65      0.72        48\n              Food       0.76      0.86      0.81        37\n               AMA       0.40      0.25      0.31         8\n\n          accuracy                           0.81       448\n         macro avg       0.79      0.78      0.77       448\n      weighted avg       0.83      0.81      0.81       448\n\nFlair Detection using Body as Feature\nResults of Naive Bayes Classifier\naccuracy 0.28125\n                    precision    recall  f1-score   support\n\n          AskIndia       0.83      0.44      0.58        34\n     Non-Political       0.09      0.97      0.16        30\n     [R]eddiquette       0.58      0.50      0.54        44\n         Scheduled       0.73      0.21      0.33        38\n       Photography       1.00      0.20      0.33        45\nScience/Technology       0.88      0.30      0.44        47\n          Politics       1.00      0.03      0.06        34\n  Business/Finance       1.00      0.03      0.05        37\n    Policy/Economy       1.00      0.24      0.39        46\n            Sports       1.00      0.08      0.15        48\n              Food       0.80      0.32      0.46        37\n               AMA       0.00      0.00      0.00         8\n\n          accuracy                           0.28       448\n         macro avg       0.74      0.28      0.29       448\n      weighted avg       0.81      0.28      0.32       448\n\nResults of Linear Support Vector Machine\naccuracy 0.41964285714285715\n                    precision    recall  f1-score   support\n\n          AskIndia       0.81      0.50      0.62        34\n     Non-Political       0.69      0.83      0.76        30\n     [R]eddiquette       0.77      0.55      0.64        44\n         Scheduled       0.59      0.26      0.36        38\n       Photography       0.88      0.33      0.48        45\nScience/Technology       1.00      0.28      0.43        47\n          Politics       0.80      0.12      0.21        34\n  Business/Finance       0.14      0.97      0.24        37\n    Policy/Economy       1.00      0.30      0.47        46\n            Sports       1.00      0.23      0.37        48\n              Food       0.83      0.41      0.55        37\n               AMA       1.00      0.50      0.67         8\n\n          accuracy                           0.42       448\n         macro avg       0.79      0.44      0.48       448\n      weighted avg       0.80      0.42      0.47       448\n\nResults of Logistic Regression\naccuracy 0.41964285714285715\n                    precision    recall  f1-score   support\n\n          AskIndia       0.80      0.47      0.59        34\n     Non-Political       0.62      0.83      0.71        30\n     [R]eddiquette       0.92      0.55      0.69        44\n         Scheduled       0.71      0.26      0.38        38\n       Photography       0.89      0.36      0.51        45\nScience/Technology       1.00      0.34      0.51        47\n          Politics       0.13      1.00      0.23        34\n  Business/Finance       1.00      0.05      0.10        37\n    Policy/Economy       1.00      0.30      0.47        46\n            Sports       1.00      0.25      0.40        48\n              Food       0.88      0.41      0.56        37\n               AMA       0.80      0.50      0.62         8\n\n          accuracy                           0.42       448\n         macro avg       0.81      0.44      0.48       448\n      weighted avg       0.84      0.42      0.47       448\n\nResults of Random Forest\naccuracy 0.41964285714285715\n                    precision    recall  f1-score   support\n\n          AskIndia       1.00      0.47      0.64        34\n     Non-Political       0.58      0.83      0.68        30\n     [R]eddiquette       0.86      0.55      0.67        44\n         Scheduled       0.80      0.32      0.45        38\n       Photography       1.00      0.36      0.52        45\nScience/Technology       1.00      0.34      0.51        47\n          Politics       0.12      0.94      0.21        34\n  Business/Finance       0.67      0.05      0.10        37\n    Policy/Economy       1.00      0.30      0.47        46\n            Sports       1.00      0.25      0.40        48\n              Food       0.94      0.41      0.57        37\n               AMA       0.80      0.50      0.62         8\n\n          accuracy                           0.42       448\n         macro avg       0.81      0.44      0.49       448\n      weighted avg       0.84      0.42      0.48       448\n\nResults of MLP Classifier\naccuracy 0.41964285714285715\n                    precision    recall  f1-score   support\n\n          AskIndia       0.73      0.47      0.57        34\n     Non-Political       0.86      0.83      0.85        30\n     [R]eddiquette       0.92      0.50      0.65        44\n         Scheduled       0.50      0.32      0.39        38\n       Photography       1.00      0.36      0.52        45\nScience/Technology       1.00      0.30      0.46        47\n          Politics       0.80      0.12      0.21        34\n  Business/Finance       0.14      0.97      0.24        37\n    Policy/Economy       0.82      0.30      0.44        46\n            Sports       0.75      0.25      0.38        48\n              Food       0.87      0.35      0.50        37\n               AMA       0.80      0.50      0.62         8\n\n          accuracy                           0.42       448\n         macro avg       0.77      0.44      0.48       448\n      weighted avg       0.77      0.42      0.47       448\n\nResults of XGB Classifier\naccuracy 0.421875\n                    precision    recall  f1-score   support\n\n          AskIndia       1.00      0.47      0.64        34\n     Non-Political       0.76      0.83      0.79        30\n     [R]eddiquette       0.92      0.55      0.69        44\n         Scheduled       0.81      0.34      0.48        38\n       Photography       0.73      0.36      0.48        45\nScience/Technology       1.00      0.34      0.51        47\n          Politics       0.12      0.94      0.21        34\n  Business/Finance       0.67      0.05      0.10        37\n    Policy/Economy       0.93      0.30      0.46        46\n            Sports       1.00      0.29      0.45        48\n              Food       0.79      0.41      0.54        37\n               AMA       0.67      0.25      0.36         8\n\n          accuracy                           0.42       448\n         macro avg       0.78      0.43      0.48       448\n      weighted avg       0.81      0.42      0.48       448\n\nFlair Detection using URL as Feature\nResults of Naive Bayes Classifier\naccuracy 0.6919642857142857\n                    precision    recall  f1-score   support\n\n          AskIndia       1.00      0.65      0.79        34\n     Non-Political       0.28      1.00      0.44        30\n     [R]eddiquette       0.97      0.70      0.82        44\n         Scheduled       0.96      0.71      0.82        38\n       Photography       0.89      0.69      0.78        45\nScience/Technology       0.62      0.81      0.70        47\n          Politics       0.68      0.88      0.77        34\n  Business/Finance       0.87      0.73      0.79        37\n    Policy/Economy       0.88      0.50      0.64        46\n            Sports       0.72      0.58      0.64        48\n              Food       1.00      0.62      0.77        37\n               AMA       0.00      0.00      0.00         8\n\n          accuracy                           0.69       448\n         macro avg       0.74      0.66      0.66       448\n      weighted avg       0.80      0.69      0.71       448\n\nResults of Linear Support Vector Machine\naccuracy 0.8102678571428571\n                    precision    recall  f1-score   support\n\n          AskIndia       1.00      0.71      0.83        34\n     Non-Political       0.41      0.93      0.57        30\n     [R]eddiquette       0.97      0.70      0.82        44\n         Scheduled       0.97      0.79      0.87        38\n       Photography       0.91      0.69      0.78        45\nScience/Technology       0.71      0.83      0.76        47\n          Politics       0.76      0.94      0.84        34\n  Business/Finance       0.89      0.84      0.86        37\n    Policy/Economy       0.95      0.87      0.91        46\n            Sports       0.88      0.88      0.88        48\n              Food       0.97      0.78      0.87        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.81       448\n         macro avg       0.86      0.81      0.82       448\n      weighted avg       0.86      0.81      0.82       448\n\nResults of Logistic Regression\naccuracy 0.8169642857142857\n                    precision    recall  f1-score   support\n\n          AskIndia       0.89      0.74      0.81        34\n     Non-Political       0.41      0.93      0.57        30\n     [R]eddiquette       0.92      0.75      0.83        44\n         Scheduled       1.00      0.79      0.88        38\n       Photography       0.94      0.73      0.83        45\nScience/Technology       0.76      0.83      0.80        47\n          Politics       0.84      0.91      0.87        34\n  Business/Finance       0.89      0.84      0.86        37\n    Policy/Economy       0.87      0.87      0.87        46\n            Sports       0.91      0.88      0.89        48\n              Food       0.97      0.76      0.85        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.82       448\n         macro avg       0.85      0.81      0.82       448\n      weighted avg       0.86      0.82      0.83       448\n\nResults of Random Forest\naccuracy 0.8191964285714286\n                    precision    recall  f1-score   support\n\n          AskIndia       0.83      0.74      0.78        34\n     Non-Political       0.41      0.93      0.57        30\n     [R]eddiquette       0.97      0.75      0.85        44\n         Scheduled       1.00      0.79      0.88        38\n       Photography       1.00      0.69      0.82        45\nScience/Technology       0.87      0.83      0.85        47\n          Politics       0.76      0.94      0.84        34\n  Business/Finance       0.74      0.84      0.78        37\n    Policy/Economy       0.95      0.87      0.91        46\n            Sports       0.91      0.88      0.89        48\n              Food       0.97      0.81      0.88        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.82       448\n         macro avg       0.86      0.82      0.82       448\n      weighted avg       0.87      0.82      0.83       448\n\nResults of MLP Classifier\naccuracy 0.8214285714285714\n                    precision    recall  f1-score   support\n\n          AskIndia       1.00      0.74      0.85        34\n     Non-Political       0.93      0.87      0.90        30\n     [R]eddiquette       0.97      0.75      0.85        44\n         Scheduled       1.00      0.79      0.88        38\n       Photography       0.49      0.84      0.62        45\nScience/Technology       1.00      0.79      0.88        47\n          Politics       0.74      0.94      0.83        34\n  Business/Finance       0.94      0.84      0.89        37\n    Policy/Economy       0.90      0.83      0.86        46\n            Sports       0.78      0.88      0.82        48\n              Food       0.81      0.81      0.81        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.82       448\n         macro avg       0.87      0.82      0.83       448\n      weighted avg       0.86      0.82      0.83       448\n\nResults of XGB Classifier\naccuracy 0.4799107142857143\n                    precision    recall  f1-score   support\n\n          AskIndia       0.75      0.09      0.16        34\n     Non-Political       0.15      0.93      0.26        30\n     [R]eddiquette       0.71      0.27      0.39        44\n         Scheduled       0.89      0.42      0.57        38\n       Photography       0.62      0.29      0.39        45\nScience/Technology       0.50      0.49      0.49        47\n          Politics       0.76      0.85      0.81        34\n  Business/Finance       0.70      0.70      0.70        37\n    Policy/Economy       0.92      0.48      0.63        46\n            Sports       0.88      0.62      0.73        48\n              Food       0.63      0.32      0.43        37\n               AMA       1.00      0.12      0.22         8\n\n          accuracy                           0.48       448\n         macro avg       0.71      0.47      0.48       448\n      weighted avg       0.70      0.48      0.51       448\n\nFlair Detection using Comments as Feature\nResults of Naive Bayes Classifier\naccuracy 0.7544642857142857\n                    precision    recall  f1-score   support\n\n          AskIndia       0.78      0.82      0.80        34\n     Non-Political       0.36      0.80      0.49        30\n     [R]eddiquette       0.94      0.68      0.79        44\n         Scheduled       0.83      0.92      0.88        38\n       Photography       1.00      0.38      0.55        45\nScience/Technology       1.00      0.87      0.93        47\n          Politics       0.57      0.85      0.68        34\n  Business/Finance       0.64      0.95      0.76        37\n    Policy/Economy       0.92      0.74      0.82        46\n            Sports       1.00      0.62      0.77        48\n              Food       0.86      0.86      0.86        37\n               AMA       1.00      0.38      0.55         8\n\n          accuracy                           0.75       448\n         macro avg       0.82      0.74      0.74       448\n      weighted avg       0.84      0.75      0.76       448\n\nResults of Linear Support Vector Machine\naccuracy 0.8660714285714286\n                    precision    recall  f1-score   support\n\n          AskIndia       0.91      0.88      0.90        34\n     Non-Political       0.80      0.80      0.80        30\n     [R]eddiquette       0.95      0.89      0.92        44\n         Scheduled       0.88      0.97      0.93        38\n       Photography       1.00      0.56      0.71        45\nScience/Technology       0.96      0.94      0.95        47\n          Politics       0.86      0.91      0.89        34\n  Business/Finance       0.79      0.89      0.84        37\n    Policy/Economy       0.88      0.91      0.89        46\n            Sports       0.70      0.90      0.79        48\n              Food       0.89      0.92      0.91        37\n               AMA       1.00      0.75      0.86         8\n\n          accuracy                           0.87       448\n         macro avg       0.88      0.86      0.86       448\n      weighted avg       0.88      0.87      0.86       448\n\nResults of Logistic Regression\naccuracy 0.8705357142857143\n                    precision    recall  f1-score   support\n\n          AskIndia       0.93      0.82      0.87        34\n     Non-Political       0.64      0.83      0.72        30\n     [R]eddiquette       0.98      0.91      0.94        44\n         Scheduled       0.93      0.97      0.95        38\n       Photography       0.65      0.73      0.69        45\nScience/Technology       1.00      0.94      0.97        47\n          Politics       0.94      0.85      0.89        34\n  Business/Finance       0.89      0.89      0.89        37\n    Policy/Economy       0.86      0.93      0.90        46\n            Sports       0.98      0.83      0.90        48\n              Food       0.86      0.86      0.86        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.87       448\n         macro avg       0.88      0.86      0.87       448\n      weighted avg       0.88      0.87      0.87       448\n\nResults of Random Forest\naccuracy 0.8928571428571429\n                    precision    recall  f1-score   support\n\n          AskIndia       0.88      0.88      0.88        34\n     Non-Political       0.81      0.83      0.82        30\n     [R]eddiquette       0.98      0.91      0.94        44\n         Scheduled       0.93      0.97      0.95        38\n       Photography       0.70      0.82      0.76        45\nScience/Technology       0.96      0.94      0.95        47\n          Politics       0.89      0.91      0.90        34\n  Business/Finance       0.94      0.89      0.92        37\n    Policy/Economy       0.88      0.93      0.91        46\n            Sports       0.98      0.83      0.90        48\n              Food       0.94      0.92      0.93        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.89       448\n         macro avg       0.89      0.88      0.89       448\n      weighted avg       0.90      0.89      0.89       448\n\nResults of MLP Classifier\naccuracy 0.8459821428571429\n                    precision    recall  f1-score   support\n\n          AskIndia       0.96      0.76      0.85        34\n     Non-Political       0.81      0.83      0.82        30\n     [R]eddiquette       0.97      0.86      0.92        44\n         Scheduled       0.95      0.92      0.93        38\n       Photography       0.54      0.89      0.67        45\nScience/Technology       0.80      0.94      0.86        47\n          Politics       0.82      0.79      0.81        34\n  Business/Finance       1.00      0.78      0.88        37\n    Policy/Economy       0.89      0.89      0.89        46\n            Sports       0.97      0.75      0.85        48\n              Food       0.97      0.86      0.91        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.85       448\n         macro avg       0.88      0.84      0.85       448\n      weighted avg       0.88      0.85      0.85       448\n\nResults of XGB Classifier\naccuracy 0.8861607142857143\n                    precision    recall  f1-score   support\n\n          AskIndia       0.94      0.88      0.91        34\n     Non-Political       0.83      0.83      0.83        30\n     [R]eddiquette       0.95      0.86      0.90        44\n         Scheduled       0.92      0.92      0.92        38\n       Photography       0.60      0.87      0.71        45\nScience/Technology       0.96      0.91      0.93        47\n          Politics       0.94      0.91      0.93        34\n  Business/Finance       0.94      0.89      0.92        37\n    Policy/Economy       0.93      0.93      0.93        46\n            Sports       1.00      0.83      0.91        48\n              Food       0.92      0.92      0.92        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.89       448\n         macro avg       0.90      0.88      0.88       448\n      weighted avg       0.90      0.89      0.89       448\n\nFlair Detection using Combined Features\nResults of Naive Bayes Classifier\naccuracy 0.8125\n                    precision    recall  f1-score   support\n\n          AskIndia       0.90      0.79      0.84        34\n     Non-Political       0.39      0.83      0.53        30\n     [R]eddiquette       0.92      0.77      0.84        44\n         Scheduled       0.89      0.87      0.88        38\n       Photography       1.00      0.62      0.77        45\nScience/Technology       0.98      0.94      0.96        47\n          Politics       0.64      1.00      0.78        34\n  Business/Finance       0.80      0.86      0.83        37\n    Policy/Economy       0.92      0.76      0.83        46\n            Sports       1.00      0.71      0.83        48\n              Food       0.90      0.95      0.92        37\n               AMA       1.00      0.38      0.55         8\n\n          accuracy                           0.81       448\n         macro avg       0.86      0.79      0.80       448\n      weighted avg       0.87      0.81      0.82       448\n\nResults of Linear Support Vector Machine\naccuracy 0.9285714285714286\n                    precision    recall  f1-score   support\n\n          AskIndia       0.91      0.91      0.91        34\n     Non-Political       0.81      0.83      0.82        30\n     [R]eddiquette       0.95      0.91      0.93        44\n         Scheduled       0.90      0.95      0.92        38\n       Photography       0.95      0.89      0.92        45\nScience/Technology       1.00      0.98      0.99        47\n          Politics       0.87      1.00      0.93        34\n  Business/Finance       0.92      0.89      0.90        37\n    Policy/Economy       0.90      0.98      0.94        46\n            Sports       1.00      0.90      0.95        48\n              Food       0.95      1.00      0.97        37\n               AMA       1.00      0.75      0.86         8\n\n          accuracy                           0.93       448\n         macro avg       0.93      0.92      0.92       448\n      weighted avg       0.93      0.93      0.93       448\n\nResults of Logistic Regression\naccuracy 0.9308035714285714\n                    precision    recall  f1-score   support\n\n          AskIndia       1.00      0.85      0.92        34\n     Non-Political       0.76      0.87      0.81        30\n     [R]eddiquette       0.95      0.89      0.92        44\n         Scheduled       0.95      0.95      0.95        38\n       Photography       0.85      0.91      0.88        45\nScience/Technology       1.00      1.00      1.00        47\n          Politics       0.94      0.97      0.96        34\n  Business/Finance       0.89      0.89      0.89        37\n    Policy/Economy       0.92      1.00      0.96        46\n            Sports       1.00      0.92      0.96        48\n              Food       0.97      1.00      0.99        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.93       448\n         macro avg       0.93      0.92      0.92       448\n      weighted avg       0.93      0.93      0.93       448\n\nResults of Random Forest\naccuracy 0.9419642857142857\n                    precision    recall  f1-score   support\n\n          AskIndia       0.94      0.85      0.89        34\n     Non-Political       0.81      0.87      0.84        30\n     [R]eddiquette       0.93      0.93      0.93        44\n         Scheduled       0.95      0.95      0.95        38\n       Photography       1.00      0.96      0.98        45\nScience/Technology       0.92      1.00      0.96        47\n          Politics       0.89      1.00      0.94        34\n  Business/Finance       1.00      0.95      0.97        37\n    Policy/Economy       0.96      0.96      0.96        46\n            Sports       0.98      0.92      0.95        48\n              Food       0.97      1.00      0.99        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.94       448\n         macro avg       0.93      0.93      0.93       448\n      weighted avg       0.94      0.94      0.94       448\n\nResults of MLP Classifier\naccuracy 0.8660714285714286\n                    precision    recall  f1-score   support\n\n          AskIndia       0.93      0.79      0.86        34\n     Non-Political       0.58      0.87      0.69        30\n     [R]eddiquette       0.85      0.80      0.82        44\n         Scheduled       0.94      0.87      0.90        38\n       Photography       0.85      0.87      0.86        45\nScience/Technology       0.83      0.96      0.89        47\n          Politics       0.94      0.91      0.93        34\n  Business/Finance       1.00      0.78      0.88        37\n    Policy/Economy       0.89      0.87      0.88        46\n            Sports       1.00      0.88      0.93        48\n              Food       0.83      0.95      0.89        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.87       448\n         macro avg       0.88      0.86      0.86       448\n      weighted avg       0.88      0.87      0.87       448\n\nResults of XGB Classifier\naccuracy 0.9464285714285714\n                    precision    recall  f1-score   support\n\n          AskIndia       0.87      0.79      0.83        34\n     Non-Political       0.93      0.93      0.93        30\n     [R]eddiquette       0.93      0.93      0.93        44\n         Scheduled       1.00      0.95      0.97        38\n       Photography       1.00      0.96      0.98        45\nScience/Technology       0.96      1.00      0.98        47\n          Politics       0.89      1.00      0.94        34\n  Business/Finance       0.95      0.95      0.95        37\n    Policy/Economy       0.96      1.00      0.98        46\n            Sports       1.00      0.92      0.96        48\n              Food       0.90      1.00      0.95        37\n               AMA       0.86      0.75      0.80         8\n\n          accuracy                           0.95       448\n         macro avg       0.94      0.93      0.93       448\n      weighted avg       0.95      0.95      0.95       448\n\n"
    }
   ],
   "source": [
    "cat = data.flair\n",
    "\n",
    "V = data.combined_features\n",
    "W = data.comment\n",
    "X = data.title\n",
    "Y = data.body\n",
    "Z = data.url\n",
    "\n",
    "print(\"Flair Detection using Title as Feature\")\n",
    "train_test(X,cat)\n",
    "print(\"Flair Detection using Body as Feature\")\n",
    "train_test(Y,cat)\n",
    "print(\"Flair Detection using URL as Feature\")\n",
    "train_test(Z,cat)\n",
    "print(\"Flair Detection using Comments as Feature\")\n",
    "train_test(W,cat)\n",
    "print(\"Flair Detection using Combined Features\")\n",
    "train_test(V,cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "X_train, X_test, y_train, y_test = train_test_split(V, cat, test_size=0.2, random_state = 42)\n",
    "model = Pipeline([('vect', CountVectorizer()),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', xgb.sklearn.XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7,objective='multi:softmax')),\n",
    "                  ])\n",
    "XGB = model.fit(X_train, y_train)\n",
    "# pickle.dump(XGB,open(\"xgb.bin\",'wb'))\n",
    "# y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(XGB, open('xgb.bin', 'wb'))\n",
    "\n",
    "# to load the saved model\n",
    "bst = joblib.load(open('xgb.bin', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model and checking the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Politics'"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "import pickle\n",
    "import logging\n",
    "import gensim\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "import xgboost as xgb\n",
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text\n",
    "          ]\n",
    "\n",
    "def clean(s):\n",
    "    s = s.lower()\n",
    "    s = utils.to_unicode(s)\n",
    "    for f in filters:\n",
    "        s = f(s)\n",
    "    return s\n",
    "\n",
    "model = bst\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(client_id = \"########\",\n",
    "\t\t\t\t\tclient_secret = \"#######\",\n",
    "\t\t\t\t\tuser_agent = \"#######\",\n",
    "\t\t\t\t\tusername = \"######\",\n",
    "\t\t\t\t\tpassword = \"######\")\n",
    "\n",
    "def prediction(url):\n",
    "\tsubmission = reddit.submission(url = url)\n",
    "\tdata = {}\n",
    "\tdata[\"title\"] = str(submission.title)\n",
    "\tdata[\"url\"] = str(submission.url)\n",
    "\tdata[\"body\"] = str(submission.selftext)\n",
    "\n",
    "\tsubmission.comments.replace_more(limit=None)\n",
    "\tcomment = ''\n",
    "\tcount = 0\n",
    "\tfor top_level_comment in submission.comments:\n",
    "\t\tcomment = comment + ' ' + top_level_comment.body\n",
    "\t\tcount+=1\n",
    "\t\tif(count > 10):\n",
    "\t\t \tbreak\n",
    "\t\t\n",
    "\tdata[\"comment\"] = str(comment)\n",
    "\n",
    "\tdata['title'] = clean(str(data['title']))\n",
    "\tdata['body'] = clean(str(data['body']))\n",
    "\tdata['comment'] = clean(str(data['comment']))\n",
    "    \n",
    "\tcombined_features = data[\"title\"] + data[\"comment\"] + data[\"body\"] + data[\"url\"]\n",
    "\n",
    "\treturn str(model.predict([combined_features]))[2:-2]\n",
    " \n",
    "prediction(\"https://www.reddit.com/r/india/comments/d1m9ld/iran_removes_antiindia_banners_from_pak_consulate/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}